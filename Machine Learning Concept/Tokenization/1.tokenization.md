# TOKENIZATION

It is the process of converting sentences into token form.
A token is the smallest unit of text that helps the model in understanding patterns.

Example:

```I love AI  --> Tokenization --> ["I", "love", "AI"]```

### Why do we need tokenization?

Because a computer cannot fully understand a sentence. Therefore, it needs to break the sentence into smaller segmented units.

## Number Encoding

After being segmented into small units, the tokens are converted into numbers called token IDs. This process is called number encoding or token mapping.

### Why do we need token mapping?

We need token mapping or number encoding because even after segmentation, the model still cannot understand text directly. It requires numerical form so that it can process the input efficiently.

So, token mapping is a step that converts human language into something the model can mathematically process.

### Why do we say "mathematical processing"?

Let’s take a simple example of ChatGPT.
If you type:
```"I love AI"```

The model cannot directly understand:

1.Feelings

2.Love towards AI

3.Intention

4.Emotion

### Then how does ChatGPT give an appropriate response?

ChatGPT does not understand feelings, intentions, emotions, or love.
Instead, it uses calculations on numbers rather than understanding words like a human.

The process is:

Breaks the sentence into tokens

Converts tokens into numbers

Performs complex calculations (mainly probability calculations)

Predicts the most likely response based on input patterns

### What calculations happen here?

The input, which has been converted into numerical form, goes through mathematical processes such as probability calculations on the embredding vector 
## Text
 **→ Tokens**
 **→ Token IDs**
 **→ Embedding vectors**
 **→ Neural network calculations** //[Neural_Network_Calculation](file:///C:/Users/ashay/OneDrive/Desktop/SACRIFICES%20OF%20COMFORT/Machine-Learning/ML%20Concept/Neural%20Network%20Calculation/Neural_Network_Calculation.md) ctrl+click 
 **→ Probability of next word**
 **→ Response**
Based on these calculations and the data it was trained on, ChatGPT predicts the most likely response.

**It recognises patterns but does not understand meaning**

###### How does token mapping work? How do words get allotted numbers? 

